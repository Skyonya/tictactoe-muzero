{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0747d8",
   "metadata": {},
   "source": [
    "# State of tictactoe game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b013c",
   "metadata": {},
   "source": [
    "This code defines a class `State` representing the state of a Tic-Tac-Toe game board.\n",
    "\n",
    "1. **Initialization**: \n",
    "    - `__init__()` initializes the board dimensions, win streak length, the board itself (initialized as an empty numpy array), current player color, winning color, and a record of actions.\n",
    "   \n",
    "   \n",
    "2. **String Representations**: \n",
    "    - `action2str()` converts a numerical action to its corresponding string representation (e.g., 0 to 'A1').\n",
    "    - `str2action()` converts a string representation of an action to its numerical counterpart.\n",
    "    - `record_string()` generates a string representation of the record of actions taken.\n",
    "\n",
    "\n",
    "3. **Printable Representation**:\n",
    "    - `__str__()` returns a string representing the current state of the board, with row and column labels, and marks for player positions.\n",
    "\n",
    "\n",
    "4. **Game Mechanics**:\n",
    "    - `play()` updates the board state with a given action, checks for win conditions, and switches player turns.\n",
    "    - `terminal()` checks if the game has ended either by a win or by filling the board.\n",
    "    - `terminal_reward()` returns the reward for the terminal state.\n",
    "    - `legal_actions()` returns a list of legal actions available in the current state.\n",
    "\n",
    "\n",
    "5. **Neural Network Features**:\n",
    "    - `feature()` generates an input tensor representing the current state of the board for a neural network.\n",
    "    - `action_feature()` generates an input tensor representing a single action for a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e58e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class State:\n",
    "    '''Board implementation of Tic-Tac-Toe'''\n",
    "    def __init__(self, width=3, height=3, winstreak=3):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.winstreak = winstreak\n",
    "        self.board = np.zeros((height, width)) # (y, x)\n",
    "        self.color = 1\n",
    "        self.win_color = 0\n",
    "        self.record = []\n",
    "\n",
    "    def action2str(self, a):\n",
    "        return f\"{chr(65 + a // self.width)}{a % self.width + 1}\"\n",
    "\n",
    "    def str2action(self, s):\n",
    "        return (ord(s[0]) - 65) * self.width + int(s[1]) - 1\n",
    "\n",
    "    def record_string(self):\n",
    "        return ' '.join([self.action2str(a) for a in self.record])\n",
    "\n",
    "    def __str__(self):\n",
    "        s = '  ' + ' '.join(str(i + 1) for i in range(self.width)) + '\\n'\n",
    "        for i in range(self.height):\n",
    "            s += chr(65 + i) + ' ' + ' '.join(['_' if cell == 0 else ('O' if cell == 1 else 'X') for cell in self.board[i]]) + '\\n'\n",
    "        s += 'record = ' + self.record_string()\n",
    "        return s\n",
    "\n",
    "    def play(self, action):\n",
    "        if isinstance(action, str):\n",
    "            for astr in action.split():\n",
    "                self.play(self.str2action(astr))\n",
    "            return self\n",
    "\n",
    "        x, y = action % self.width, action // self.width\n",
    "        self.board[y, x] = self.color\n",
    "\n",
    "        for direction in [(0, 1), (1, 0), (1, 1), (1, -1)]:\n",
    "            dx, dy = direction\n",
    "            count = 0\n",
    "            while 0 <= x < self.width and 0 <= y < self.height and self.board[y, x] == self.color:\n",
    "                count += 1\n",
    "                if count >= self.winstreak:\n",
    "                    self.win_color = self.color\n",
    "                    break\n",
    "                x += dx\n",
    "                y += dy\n",
    "            x, y = action % self.width, action // self.width\n",
    "\n",
    "            dx, dy = -direction[0], -direction[1]\n",
    "            count = 0  # Adjusting for the initial stone counted twice\n",
    "            while 0 <= x < self.width and 0 <= y < self.height and self.board[y, x] == self.color:\n",
    "                count += 1\n",
    "                if count >= self.winstreak:\n",
    "                    self.win_color = self.color\n",
    "                    break\n",
    "                x += dx\n",
    "                y += dy\n",
    "\n",
    "        self.color = -self.color\n",
    "        self.record.append(action)\n",
    "        return self\n",
    "\n",
    "    def terminal(self):\n",
    "        return self.win_color != 0 or len(self.record) == self.width * self.height\n",
    "\n",
    "    def terminal_reward(self):\n",
    "        return self.win_color if self.color == 1 else -self.win_color\n",
    "\n",
    "    def legal_actions(self):\n",
    "        return [a for a in range(self.width * self.height) if self.board[a // self.width, a % self.width] == 0]\n",
    "    \n",
    "    def feature(self):\n",
    "        # input tensor for neural net (state)\n",
    "        return np.stack([self.board == self.color, self.board == -self.color]).astype(np.float32)\n",
    "\n",
    "    def action_feature(self, action):\n",
    "        # input tensor for neural net (action)\n",
    "        a = np.zeros((1, self.height, self.width), dtype=np.float32)\n",
    "        a[0, action // self.width, action % self.width] = 1\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b2d5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 2 3\n",
      "A O O _\n",
      "B X X _\n",
      "C _ _ _\n",
      "record = A1 B1 A2 B2\n",
      "[2, 5, 6, 7, 8]\n",
      "False\n",
      "\n",
      "  1 2 3\n",
      "A O X X\n",
      "B _ O _\n",
      "C _ _ _\n",
      "record = A1 A2 B2 A3\n",
      "[3, 5, 6, 7, 8]\n",
      "False\n",
      "\n",
      "  1 2 3\n",
      "A O O O\n",
      "B X X _\n",
      "C _ _ _\n",
      "record = A1 B1 A2 B2 A3\n",
      "[5, 6, 7, 8]\n",
      "True\n",
      "\n",
      "  1 2 3\n",
      "A O X X\n",
      "B _ O _\n",
      "C _ _ O\n",
      "record = A1 A2 B2 A3 C3\n",
      "[3, 5, 6, 7]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "\n",
    "state = State().play('A1 B1 A2 B2')\n",
    "print(state)\n",
    "print(state.legal_actions())\n",
    "print(state.terminal())\n",
    "print()\n",
    "\n",
    "state = State().play('A1 A2 B2 A3')\n",
    "print(state)\n",
    "print(state.legal_actions())\n",
    "print(state.terminal())\n",
    "print()\n",
    "\n",
    "state = State().play('A1 B1 A2 B2 A3')\n",
    "print(state)\n",
    "print(state.legal_actions())\n",
    "print(state.terminal())\n",
    "print()\n",
    "\n",
    "state = State().play('A1 A2 B2 A3 C3')\n",
    "print(state)\n",
    "print(state.legal_actions())\n",
    "print(state.terminal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53711827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 2 3\n",
      "A O O O\n",
      "B X X _\n",
      "C _ _ _\n",
      "record = A1 B1 A2 B2 A3\n",
      "True\n",
      "[5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "state = State().play('A1 B1 A2 B2 A3')\n",
    "print(state)\n",
    "print(state.terminal())\n",
    "print(state.legal_actions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c1e96",
   "metadata": {},
   "source": [
    "# Neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f99a75c",
   "metadata": {},
   "source": [
    "This code defines a convolutional neural network architecture using PyTorch.\n",
    "\n",
    "1. **Convolutional Layer** (`Conv` class):\n",
    "   - `__init__()`: Initializes a 2D convolutional layer with the specified number of input and output channels (filters), kernel size, and optional batch normalization.\n",
    "   - `forward()`: Performs the forward pass through the convolutional layer, applying convolution and batch normalization if specified.\n",
    "\n",
    "\n",
    "2. **Residual Block** (`ResidualBlock` class):\n",
    "   - `__init__()`: Initializes a residual block consisting of two convolutional layers with the same number of input and output channels, followed by ReLU activation.\n",
    "   - `forward()`: Defines the forward pass of the residual block, adding the input to the output of the convolutional layer and applying ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77a924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, filters0, filters1, kernel_size, bn=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(filters0, filters1, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
    "        self.bn = None\n",
    "        if bn:\n",
    "            self.bn = nn.BatchNorm2d(filters1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            h = self.bn(h)\n",
    "        return h\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(filters, filters, 3, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(x + (self.conv(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc5391",
   "metadata": {},
   "source": [
    "This code defines a neural network architecture for a reinforcement learning agent using PyTorch.\n",
    "\n",
    "1. **Representation Network** (`Representation` class):\n",
    "   - This class converts observations into an inner abstract state.\n",
    "   - `__init__()`: Initializes the network with convolutional layers and residual blocks.\n",
    "   - `forward()`: Performs the forward pass through the network, applying ReLU activation after each convolutional layer.\n",
    "\n",
    "\n",
    "2. **Prediction Network** (`Prediction` class):\n",
    "   - This network predicts policy (action probabilities) and value (expected outcome) from the inner abstract state.\n",
    "   - `__init__()`: Initializes the network with convolutional and fully connected layers.\n",
    "   - `forward()`: Performs the forward pass, returning softmax probabilities for actions and a value estimate.\n",
    "\n",
    "\n",
    "3. **Dynamics Network** (`Dynamics` class):\n",
    "   - This network models abstract state transitions given the inner abstract state and an action.\n",
    "   - `__init__()`: Initializes the network with convolutional layers and residual blocks.\n",
    "   - `forward()`: Performs the forward pass, concatenating the inner abstract state and action before processing.\n",
    "\n",
    "\n",
    "4. **Overall Network** (`Net` class):\n",
    "   - Combines the representation, prediction, and dynamics networks into a single model.\n",
    "   - `__init__()`: Initializes the entire network by creating instances of the representation, prediction, and dynamics networks.\n",
    "   - `predict()`: Predicts policy and value from the original state and a sequence of actions.\n",
    "\n",
    "\n",
    "Overall, this architecture is designed for a reinforcement learning agent to learn and make decisions in an environment, where the representation network encodes observations, the prediction network estimates policy and value, and the dynamics network models state transitions. The `predict()` method combines these components to provide predictions based on the current state and action sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f64e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = 16\n",
    "num_blocks = 4\n",
    "\n",
    "class Representation(nn.Module):\n",
    "    ''' Conversion from observation to inner abstract state '''\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.board_size = self.input_shape[1] * self.input_shape[2]\n",
    "\n",
    "        self.layer0 = Conv(self.input_shape[0], num_filters, 3, bn=True)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.layer0(x))\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            rp = self(torch.from_numpy(x).unsqueeze(0))\n",
    "        return rp.cpu().numpy()[0]\n",
    "\n",
    "class Prediction(nn.Module):\n",
    "    ''' Policy and value prediction from inner abstract state '''\n",
    "    def __init__(self, action_shape):\n",
    "        super().__init__()\n",
    "        self.board_size = np.prod(action_shape[1:])\n",
    "        self.action_size = action_shape[0] * self.board_size\n",
    "\n",
    "        self.conv_p1 = Conv(num_filters, 4, 1, bn=True)\n",
    "        self.conv_p2 = Conv(4, 1, 1)\n",
    "\n",
    "        self.conv_v = Conv(num_filters, 4, 1, bn=True)\n",
    "        self.fc_v = nn.Linear(self.board_size * 4, 1, bias=False)\n",
    "\n",
    "    def forward(self, rp):\n",
    "        h_p = F.relu(self.conv_p1(rp))\n",
    "        h_p = self.conv_p2(h_p).view(-1, self.action_size)\n",
    "\n",
    "        h_v = F.relu(self.conv_v(rp))\n",
    "        h_v = self.fc_v(h_v.view(-1, self.board_size * 4))\n",
    "\n",
    "        # range of value is -1 ~ 1\n",
    "        return F.softmax(h_p, dim=-1), torch.tanh(h_v)\n",
    "\n",
    "    def inference(self, rp):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            p, v = self(torch.from_numpy(rp).unsqueeze(0))\n",
    "        return p.cpu().numpy()[0], v.cpu().numpy()[0][0]\n",
    "\n",
    "class Dynamics(nn.Module):\n",
    "    '''Abstract state transition'''\n",
    "    def __init__(self, rp_shape, act_shape):\n",
    "        super().__init__()\n",
    "        self.rp_shape = rp_shape\n",
    "        self.layer0 = Conv(rp_shape[0] + act_shape[0], num_filters, 3, bn=True)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, rp, a):\n",
    "        h = torch.cat([rp, a], dim=1)\n",
    "        h = self.layer0(h)\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, rp, a):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            rp = self(torch.from_numpy(rp).unsqueeze(0), torch.from_numpy(a).unsqueeze(0))\n",
    "        return rp.cpu().numpy()[0]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    '''Whole net'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        state = State()\n",
    "        input_shape = state.feature().shape\n",
    "        action_shape = state.action_feature(0).shape\n",
    "        rp_shape = (num_filters, *input_shape[1:])\n",
    "\n",
    "        self.representation = Representation(input_shape)\n",
    "        self.prediction = Prediction(action_shape)\n",
    "        self.dynamics = Dynamics(rp_shape, action_shape)\n",
    "\n",
    "    def predict(self, state0, path):\n",
    "        '''Predict p and v from original state and path'''\n",
    "        outputs = []\n",
    "        x = state0.feature()\n",
    "        rp = self.representation.inference(x)\n",
    "        outputs.append(self.prediction.inference(rp))\n",
    "        for action in path:\n",
    "            a = state0.action_feature(action)\n",
    "            rp = self.dynamics.inference(rp, a)\n",
    "            outputs.append(self.prediction.inference(rp))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4baa44b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 2 3\n",
      "A _ _ _\n",
      "B _ _ _\n",
      "C _ _ _\n",
      "record = \n",
      "p = \n",
      "[[[111 111 111]\n",
      "  [111 111 111]\n",
      "  [111 111 111]]]\n",
      "v =  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_net(net, state):\n",
    "    '''Display policy (p) and value (v)'''\n",
    "    print(state)\n",
    "    p, v = net.predict(state, [])[-1]\n",
    "    print('p = ')\n",
    "    print((p * 1000).astype(int).reshape((-1, *net.representation.input_shape[1:3])))\n",
    "    print('v = ', v)\n",
    "    print()\n",
    "\n",
    "#  Outputs before training\n",
    "show_net(Net(), State())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19927d",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search (MCTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a80a71",
   "metadata": {},
   "source": [
    "This code defines a class `Node` representing the search result of one abstract (or root) state in a tree search algorithm, such as Monte Carlo Tree Search (MCTS). Here's a breakdown of its components:\n",
    "\n",
    "\n",
    "1. **Initialization**: \n",
    "    - `__init__()`: Initializes a node with policy (`p`) and value (`v`) estimates for the state. It also initializes arrays to store visit counts (`n`) and cumulative action values (`q_sum`). Additionally, it maintains overall visit count (`n_all`) and cumulative value (`q_sum_all`) for the node.\n",
    "\n",
    "\n",
    "2. **Update Method**: \n",
    "    - `update()`: Updates the node statistics after selecting and evaluating an action. It takes the selected action index and the new value estimate (`q_new`) as inputs. It increments the visit count and adds the new value to the cumulative sum for the selected action. Then, it updates the overall visit count and cumulative value for the node.\n",
    "\n",
    "Overall, this class is used to maintain statistics for actions taken from a specific state during the search process, enabling informed action selection based on exploration-exploitation trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce5919a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Monte Carlo Tree Search\n",
    "\n",
    "class Node:\n",
    "    '''Search result of one abstract (or root) state'''\n",
    "    def __init__(self, p, v):\n",
    "        self.p, self.v = p, v\n",
    "        self.n, self.q_sum = np.zeros_like(p), np.zeros_like(p)\n",
    "        self.n_all, self.q_sum_all = 1, v / 2 # prior\n",
    "\n",
    "    def update(self, action, q_new):\n",
    "        # Update\n",
    "        self.n[action] += 1\n",
    "        self.q_sum[action] += q_new\n",
    "\n",
    "        # Update overall stats\n",
    "        self.n_all += 1\n",
    "        self.q_sum_all += q_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926b635c",
   "metadata": {},
   "source": [
    "This code implements a Monte Carlo Tree Search (MCTS) algorithm along with a supporting tree structure for reinforcement learning tasks. Here's a detailed breakdown:\n",
    "\n",
    "1. **Node Class**\n",
    "    - Purpose: Represents the search result of one abstract (or root) state in the tree search.\n",
    "    - `__init__()`: Initializes the node with policy (`p`) and value (`v`) estimates. Also initializes arrays to store visit counts (`n`) and cumulative action values (`q_sum`). Additionally, maintains overall visit count (`n_all`) and cumulative value (`q_sum_all`) for the node.\n",
    "    - `update()`: Updates the node statistics after selecting and evaluating an action.\n",
    "\n",
    "\n",
    "2. **Tree Class (Monte Carlo Tree Search)**\n",
    "    - Purpose: Implements the MCTS algorithm.\n",
    "    - `__init__()`: Initializes the MCTS tree with a neural network (`net`) and an empty dictionary to store nodes.\n",
    "    - `search()`: Conducts a search from the current state recursively. Updates nodes and selects actions based on exploration-exploitation trade-offs (PUCB formula).\n",
    "    - `think()`: Performs a series of MCTS simulations to determine the best action given the current state. Optionally, it can display the search progress.\n",
    "    - `pv()`: Returns the principal variation, i.e., the action sequence considered as the best.\n",
    "\n",
    "\n",
    "3. **General Comments**\n",
    "    - The MCTS algorithm iteratively builds a search tree by simulating trajectories from the current state and updating node statistics accordingly.\n",
    "    - It balances exploration (trying new actions) and exploitation (favoring actions with high estimated value) to find the most promising actions.\n",
    "    - The code is well-documented, making it clear and understandable. It's a crucial aspect, especially for complex algorithms like MCTS.\n",
    "\n",
    "Overall, this code provides a robust implementation of MCTS, which can be integrated with various reinforcement learning tasks, especially in game-playing scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8803197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "class Tree:\n",
    "    '''Monte Carlo Tree Search (MCTS)'''\n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "        self.nodes = {}\n",
    "\n",
    "    def search(self, state, path, rp, depth):\n",
    "        # Return predicted value from new state\n",
    "        key = state.record_string()\n",
    "        if len(path) > 0:\n",
    "            key += '|' + ' '.join(map(state.action2str, path))\n",
    "        if key not in self.nodes:\n",
    "            p, v = self.net.prediction.inference(rp)\n",
    "            self.nodes[key] = Node(p, v)\n",
    "            return v\n",
    "\n",
    "        # State transition by an action selected from bandit\n",
    "        node = self.nodes[key]\n",
    "        p = node.p\n",
    "        mask = np.zeros_like(p)\n",
    "        if depth == 0:\n",
    "            # Add noise to policy on the root node\n",
    "            p = 0.75 * p + 0.25 * np.random.dirichlet([0.15] * len(p))\n",
    "            # On the root node, we choose action only from legal actions\n",
    "            mask[state.legal_actions()] = 1\n",
    "            p *= mask\n",
    "            p /= p.sum() + 1e-16\n",
    "\n",
    "        n, q_sum = 1 + node.n, node.q_sum_all / node.n_all + node.q_sum\n",
    "        ucb = q_sum / n + 2.0 * np.sqrt(node.n_all) * p / n + mask * 4 # PUCB formula\n",
    "        best_action = np.argmax(ucb)\n",
    "\n",
    "        # Search next state by recursively calling this function\n",
    "        rp_next = self.net.dynamics.inference(rp, state.action_feature(best_action))\n",
    "        path.append(best_action)\n",
    "        q_new = -self.search(state, path, rp_next, depth + 1) # With the assumption of changing player by turn\n",
    "        node.update(best_action, q_new)\n",
    "\n",
    "        return q_new\n",
    "\n",
    "    def think(self, state, num_simulations, temperature = 0, show=False):\n",
    "        # End point of MCTS\n",
    "        if show:\n",
    "            print(state)\n",
    "        start, prev_time = time.time(), 0\n",
    "        for _ in range(num_simulations):\n",
    "            self.search(state, [], self.net.representation.inference(state.feature()), depth=0)\n",
    "\n",
    "            # Display search result on every second\n",
    "            if show:\n",
    "                tmp_time = time.time() - start\n",
    "                if int(tmp_time) > int(prev_time):\n",
    "                    prev_time = tmp_time\n",
    "                    root, pv = self.nodes[state.record_string()], self.pv(state)\n",
    "                    print('%.2f sec. best %s. q = %.4f. n = %d / %d. pv = %s'\n",
    "                          % (tmp_time, state.action2str(pv[0]), root.q_sum[pv[0]] / root.n[pv[0]],\n",
    "                             root.n[pv[0]], root.n_all, ' '.join([state.action2str(a) for a in pv])))\n",
    "\n",
    "        #  Return probability distribution weighted by the number of simulations\n",
    "        root = self.nodes[state.record_string()]\n",
    "        n = root.n + 1\n",
    "        n = (n / np.max(n)) ** (1 / (temperature + 1e-8))\n",
    "        return n / n.sum()\n",
    "\n",
    "    def pv(self, state):\n",
    "        # Return principal variation (action sequence which is considered as the best)\n",
    "        s, pv_seq = copy.deepcopy(state), []\n",
    "        while True:\n",
    "            key = s.record_string()\n",
    "            if key not in self.nodes or self.nodes[key].n.sum() == 0:\n",
    "                break\n",
    "            best_action = sorted([(a, self.nodes[key].n[a]) for a in s.legal_actions()], key=lambda x: -x[1])[0][0]\n",
    "            pv_seq.append(best_action)\n",
    "            s.play(best_action)\n",
    "        return pv_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da560ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 2 3\n",
      "A _ _ _\n",
      "B _ _ _\n",
      "C _ _ _\n",
      "record = \n",
      "  1 2 3\n",
      "A O O _\n",
      "B X X _\n",
      "C _ _ _\n",
      "record = A1 B1 A2 B2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search with initialized net\n",
    "\n",
    "tree = Tree(Net())\n",
    "tree.think(State(), 100, show=True)\n",
    "\n",
    "tree = Tree(Net())\n",
    "tree.think(State().play('A1 B1 A2 B2'), 200, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d9cbfa",
   "metadata": {},
   "source": [
    "# Training of neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a133fbda",
   "metadata": {},
   "source": [
    "This code defines a training function for a neural network used in reinforcement learning. Here's a breakdown of its components:\n",
    "\n",
    "1. **`gen_target()` Function**:\n",
    "   - **Purpose**: Generates inputs and targets for training by extracting information from episodes.\n",
    "   - **Inputs**: `ep` (an episode), `k` (number of steps to consider).\n",
    "   - **Output**: Returns inputs (`x`), action targets (`ax`), policy targets (`p_target`), and value targets (`v_target`) for training.\n",
    "\n",
    "\n",
    "2. **`train()` Function**:\n",
    "   - **Purpose**: Trains the neural network.\n",
    "   - **Inputs**: `episodes` (list of episodes), `net` (neural network model), `opt` (optimizer).\n",
    "   - **Steps**:\n",
    "     - Samples a batch of training data from episodes using `gen_target()`.\n",
    "     - Computes losses for policy and value predictions for each step in the batch.\n",
    "     - Updates the neural network parameters based on the total loss.\n",
    "     - Prints the average policy and value losses over the training data.\n",
    "\n",
    "Overall, this function facilitates training the neural network model for reinforcement learning tasks, optimizing it to predict policies and values that maximize rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f3dfc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 32\n",
    "num_steps = 100\n",
    "\n",
    "def gen_target(ep, k):\n",
    "    '''Generate inputs and targets for training'''\n",
    "    # path, reward, observation, action, policy\n",
    "    turn_idx = np.random.randint(len(ep[0]))\n",
    "    ps, vs, ax = [], [], []\n",
    "    for t in range(turn_idx, turn_idx + k + 1):\n",
    "        if t < len(ep[0]):\n",
    "            p = ep[4][t]\n",
    "            a = ep[3][t]\n",
    "        else: # state after finishing game\n",
    "            # p is 0 (loss is 0)\n",
    "            p = np.zeros_like(ep[4][-1])\n",
    "            # random action selection\n",
    "            a = np.zeros(np.prod(ep[3][-1].shape), dtype=np.float32)\n",
    "            a[np.random.randint(len(a))] = 1\n",
    "            a = a.reshape(ep[3][-1].shape)\n",
    "        vs.append([ep[1] if t % 2 == 0 else -ep[1]])\n",
    "        ps.append(p)\n",
    "        ax.append(a)\n",
    "        \n",
    "    return ep[2][turn_idx], ax, ps, vs\n",
    "\n",
    "def train(episodes, net, opt):\n",
    "    '''Train neural net'''\n",
    "    p_loss_sum, v_loss_sum = 0, 0\n",
    "    net.train()\n",
    "    k = 4\n",
    "    for _ in range(num_steps):\n",
    "        x, ax, p_target, v_target = zip(*[gen_target(episodes[np.random.randint(len(episodes))], k) for j in range(batch_size)])\n",
    "        x = torch.from_numpy(np.array(x))\n",
    "        ax = torch.from_numpy(np.array(ax))\n",
    "        p_target = torch.from_numpy(np.array(p_target))\n",
    "        v_target = torch.FloatTensor(np.array(v_target))\n",
    "\n",
    "        # Change the order of axis as [time step, batch, ...]\n",
    "        ax = torch.transpose(ax, 0, 1)\n",
    "        p_target = torch.transpose(p_target, 0, 1)\n",
    "        v_target = torch.transpose(v_target, 0, 1)\n",
    "\n",
    "        # Compute losses for k (+ current) steps\n",
    "        p_loss, v_loss = 0, 0\n",
    "        for t in range(k + 1):\n",
    "            rp = net.representation(x) if t == 0 else net.dynamics(rp, ax[t - 1])\n",
    "            p, v = net.prediction(rp)\n",
    "            p_loss += F.kl_div(torch.log(p), p_target[t], reduction='sum')\n",
    "            v_loss += torch.sum(((v_target[t] - v) ** 2) / 2)\n",
    "\n",
    "        p_loss_sum += p_loss.item()\n",
    "        v_loss_sum += v_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        (p_loss + v_loss).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    num_train_datum = num_steps * batch_size\n",
    "    print('p_loss %f v_loss %f' % (p_loss_sum / num_train_datum, v_loss_sum / num_train_datum))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e33d3",
   "metadata": {},
   "source": [
    "This function simulates a specified number of games (default: 100) between a neural network-based player and a random player. Here's how it works:\n",
    "\n",
    "- **Input**:\n",
    "  - `net`: The neural network model used for making decisions.\n",
    "  - `n`: The number of games to simulate.\n",
    "\n",
    "\n",
    "- **Steps**:\n",
    "  1. Initialize an empty dictionary `results` to store the outcome statistics of the games.\n",
    "  2. Iterate over each game, determining the starting player based on the game index.\n",
    "  3. Create a new game state.\n",
    "  4. Play the game until it reaches a terminal state (win, lose, or draw).\n",
    "  5. At each turn, if it's the neural network's turn, it predicts the policy for the current state and chooses the action with the highest predicted probability. Otherwise, the random player selects a random legal action.\n",
    "  6. Update the game state with the chosen action and switch the turn to the other player.\n",
    "  7. After the game ends, determine the final reward based on the terminal state and update the `results` dictionary with the outcome.\n",
    "\n",
    "\n",
    "- **Output**:\n",
    "  - `results`: A dictionary containing the frequency of different game outcomes (rewards) observed over the simulated games. Keys represent the final rewards, and values represent the number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be4ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# against random agents\n",
    "\n",
    "def vs_random(net, n=100):\n",
    "    results = {}\n",
    "    for i in range(n):\n",
    "        first_turn = i % 2 == 0\n",
    "        turn = first_turn\n",
    "        state = State()\n",
    "        while not state.terminal():\n",
    "            if turn:\n",
    "                p, _ = net.predict(state, [])[-1]\n",
    "                action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]\n",
    "            else:\n",
    "                action = np.random.choice(state.legal_actions())\n",
    "            state.play(action)\n",
    "            turn = not turn\n",
    "        r = state.terminal_reward() if turn else -state.terminal_reward()\n",
    "        results[r] = results.get(r, 0) + 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173c4876",
   "metadata": {},
   "source": [
    "# Training loop (using CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324cdb8",
   "metadata": {},
   "source": [
    "This script trains a neural network model using reinforcement learning by generating and simulating episodes of gameplay. Here's a breakdown of its components:\n",
    "\n",
    "- **Initialization**:\n",
    "  - `num_games`, `num_games_one_epoch`, and `num_simulations` define the parameters for training and simulation.\n",
    "\n",
    "\n",
    "- **Neural Network Initialization**:\n",
    "  - `net` is initialized as a neural network model.\n",
    "  - `optimizer` is initialized using stochastic gradient descent (SGD) with specified learning rate, weight decay, and momentum.\n",
    "\n",
    "\n",
    "- **Simulating Games and Training**:\n",
    "  - The script iterates over a specified number of games (`num_games`).\n",
    "  - Within each game iteration:\n",
    "    - An episode is generated by simulating gameplay using a combination of neural network predictions and random actions.\n",
    "    - The episode's outcome and details are recorded.\n",
    "    - If a certain number of games have been played (`num_games_one_epoch`), the recorded episodes are used to train the neural network using the `train()` function.\n",
    "    - The results of training and the performance of the network against random play are printed periodically.\n",
    "\n",
    "\n",
    "- **Output**:\n",
    "  - The script prints various details during execution, such as the results of battles against random play, the distribution of episode outcomes, and the progress of training.\n",
    "  - After training completes, a \"finished\" message is printed.\n",
    "\n",
    "\n",
    "This script enables the iterative training of a neural network model for reinforcement learning tasks, improving its performance through simulated gameplay experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20db7849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs_random =  [(-1, 31), (0, 41), (1, 28)]\n",
      "game 0  1  2  3  4  5  6  7  8  9  generated =  [(-1, 1), (0, 6), (1, 3)]\n",
      "p_loss 1.978555 v_loss 0.378698\n",
      "vs_random =  [(-1, 26), (0, 41), (1, 33)] sum =  [(-1, 57), (0, 82), (1, 61)]\n",
      "game 10  11  12  13  14  15  16  17  18  19  generated =  [(-1, 2), (0, 10), (1, 8)]\n",
      "p_loss 1.461495 v_loss 0.435645\n",
      "vs_random =  [(-1, 29), (0, 40), (1, 31)] sum =  [(-1, 86), (0, 122), (1, 92)]\n",
      "game 20  21  22  23  24  25  26  27  28  29  generated =  [(-1, 6), (0, 13), (1, 11)]\n",
      "p_loss 0.767429 v_loss 0.416586\n",
      "vs_random =  [(-1, 39), (0, 38), (1, 23)] sum =  [(-1, 125), (0, 160), (1, 115)]\n",
      "game 30  31  32  33  34  35  36  37  38  39  generated =  [(-1, 8), (0, 18), (1, 14)]\n",
      "p_loss 0.746043 v_loss 0.380497\n",
      "vs_random =  [(-1, 25), (0, 37), (1, 38)] sum =  [(-1, 150), (0, 197), (1, 153)]\n",
      "game 40  41  42  43  44  45  46  47  48  49  generated =  [(-1, 8), (0, 20), (1, 22)]\n",
      "p_loss 0.676101 v_loss 0.419888\n",
      "vs_random =  [(-1, 26), (0, 34), (1, 40)] sum =  [(-1, 176), (0, 231), (1, 193)]\n",
      "game 50  51  52  53  54  55  56  57  58  59  generated =  [(-1, 12), (0, 23), (1, 25)]\n",
      "p_loss 0.700599 v_loss 0.459969\n",
      "vs_random =  [(-1, 22), (0, 29), (1, 49)] sum =  [(-1, 198), (0, 260), (1, 242)]\n",
      "game 60  61  62  63  64  65  66  67  68  69  generated =  [(-1, 13), (0, 29), (1, 28)]\n",
      "p_loss 0.705644 v_loss 0.482427\n",
      "vs_random =  [(-1, 23), (0, 33), (1, 44)] sum =  [(-1, 221), (0, 293), (1, 286)]\n",
      "game 70  71  72  73  74  75  76  77  78  79  generated =  [(-1, 19), (0, 33), (1, 28)]\n",
      "p_loss 0.670786 v_loss 0.551552\n",
      "vs_random =  [(-1, 28), (0, 30), (1, 42)] sum =  [(-1, 249), (0, 323), (1, 328)]\n",
      "game 80  81  82  83  84  85  86  87  88  89  generated =  [(-1, 24), (0, 34), (1, 32)]\n",
      "p_loss 0.756636 v_loss 0.683348\n",
      "vs_random =  [(-1, 26), (0, 31), (1, 43)] sum =  [(-1, 275), (0, 354), (1, 371)]\n",
      "game 90  91  92  93  94  95  96  97  98  99  generated =  [(-1, 28), (0, 37), (1, 35)]\n",
      "p_loss 0.689040 v_loss 0.686962\n",
      "vs_random =  [(-1, 32), (0, 23), (1, 45)] sum =  [(-1, 307), (0, 377), (1, 416)]\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# Main algorithm of MuZero\n",
    "\n",
    "num_games = 100\n",
    "num_games_one_epoch = 10\n",
    "num_simulations = 40\n",
    "\n",
    "\"\"\"\n",
    "for 3x3 try:\n",
    "num_games = 500\n",
    "num_games_one_epoch = 20\n",
    "num_simulations = 40\n",
    "\"\"\"\n",
    "\n",
    "net = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=3e-4, weight_decay=3e-5, momentum=0.8)\n",
    "\n",
    "# Display battle results as {-1: lose 0: draw 1: win} (for episode generated for training, 1 means that the first player won)\n",
    "vs_random_sum = vs_random(net)\n",
    "print('vs_random = ', sorted(vs_random_sum.items()))\n",
    "\n",
    "episodes = []\n",
    "result_distribution = {1: 0, 0: 0, -1: 0}\n",
    "\n",
    "for g in range(num_games):\n",
    "    # Generate one episode\n",
    "    record, p_targets, features, action_features = [], [], [], []\n",
    "    state = State()\n",
    "    # temperature using to make policy targets from search results\n",
    "    temperature = 0.7\n",
    "\n",
    "    while not state.terminal():\n",
    "        tree = Tree(net)\n",
    "        p_target = tree.think(state, num_simulations, temperature)\n",
    "        p_targets.append(p_target)\n",
    "        features.append(state.feature())\n",
    "\n",
    "        # Select action with generated distribution, and then make a transition by that action\n",
    "        action = np.random.choice(np.arange(len(p_target)), p=p_target)\n",
    "        record.append(action)\n",
    "        action_features.append(state.action_feature(action))\n",
    "        state.play(action)\n",
    "        temperature *= 0.8\n",
    "\n",
    "    # reward seen from the first turn player\n",
    "    reward = state.terminal_reward() * (1 if len(record) % 2 == 0 else -1)\n",
    "    result_distribution[reward] += 1\n",
    "    episodes.append((record, reward, features, action_features, p_targets))\n",
    "\n",
    "    if g % num_games_one_epoch == 0:\n",
    "        print('game ', end='')\n",
    "    print(g, ' ', end='')\n",
    "\n",
    "    # Training of neural net\n",
    "    if (g + 1) % num_games_one_epoch == 0:\n",
    "        # Show the result distributiuon of generated episodes\n",
    "        print('generated = ', sorted(result_distribution.items()))\n",
    "        net = train(episodes, net, optimizer)\n",
    "        vs_random_once = vs_random(net)\n",
    "        print('vs_random = ', sorted(vs_random_once.items()), end='')\n",
    "        for r, n in vs_random_once.items():\n",
    "            vs_random_sum[r] += n\n",
    "        print(' sum = ', sorted(vs_random_sum.items()))\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e3071be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 2 3\n",
      "A _ _ _\n",
      "B _ _ _\n",
      "C _ _ _\n",
      "record = \n",
      "---------------\n",
      "player move = A1\n",
      "ai move = C1\n",
      "  1 2 3\n",
      "A O _ _\n",
      "B _ _ _\n",
      "C X _ _\n",
      "record = A1 C1\n",
      "---------------\n",
      "player move = B2\n",
      "ai move = A2\n",
      "  1 2 3\n",
      "A O X _\n",
      "B _ O _\n",
      "C X _ _\n",
      "record = A1 C1 B2 A2\n",
      "---------------\n",
      "player move = C3\n",
      "ai move = B3\n",
      "  1 2 3\n",
      "A O X _\n",
      "B _ O X\n",
      "C X _ O\n",
      "record = A1 C1 B2 A2 C3 B3\n"
     ]
    }
   ],
   "source": [
    "# player first\n",
    "\n",
    "state = State()\n",
    "tree = Tree(net)\n",
    "print(state)\n",
    "\n",
    "while not state.terminal():\n",
    "    print('---------------')\n",
    "    \n",
    "    user_action = input('player move = ')\n",
    "    state.play(user_action)\n",
    "    \n",
    "    tree.think(state, 300)\n",
    "    best_choise = tree.pv(state)\n",
    "    bot_action = state.action2str(best_choise[0])\n",
    "    print('ai move =', bot_action)\n",
    "    state.play(bot_action)\n",
    "    \n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db8bc722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "  1 2 3\n",
      "A _ _ _\n",
      "B O _ _\n",
      "C _ _ _\n",
      "record = B1\n",
      "ai move =  B1\n",
      "player move = B2\n",
      "---------------\n",
      "  1 2 3\n",
      "A O _ _\n",
      "B O X _\n",
      "C _ _ _\n",
      "record = B1 B2 A1\n",
      "ai move =  A1\n",
      "player move = C3\n",
      "---------------\n",
      "  1 2 3\n",
      "A O O _\n",
      "B O X _\n",
      "C _ _ X\n",
      "record = B1 B2 A1 C3 A2\n",
      "ai move =  A2\n",
      "player move = B2\n",
      "---------------\n",
      "  1 2 3\n",
      "A O O _\n",
      "B O X _\n",
      "C O _ X\n",
      "record = B1 B2 A1 C3 A2 B2 C1\n",
      "ai move =  C1\n",
      "player move = \n",
      "  1 2 3\n",
      "A O O _\n",
      "B O X _\n",
      "C O _ X\n",
      "record = B1 B2 A1 C3 A2 B2 C1\n"
     ]
    }
   ],
   "source": [
    "# ai first\n",
    "\n",
    "state = State()\n",
    "tree = Tree(net)\n",
    "\n",
    "while not state.terminal():\n",
    "    print('---------------')\n",
    "    \n",
    "    tree.think(state, 300)\n",
    "    best_choise = tree.pv(state)\n",
    "    bot_action = state.action2str(best_choise[0])\n",
    "    state.play(bot_action)\n",
    "    \n",
    "    print(state)\n",
    "    \n",
    "    print('ai move = ', bot_action)\n",
    "    user_action = input('player move = ')\n",
    "    state.play(user_action)\n",
    "    \n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c562ad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# cuda check\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55759c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "242.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
